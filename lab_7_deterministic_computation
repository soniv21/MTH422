M <- 10 # size of binomial
n <- 100 # sample size

theta.true <- 0.6

set.seed(100)

Y <- rbinom(n, M, theta.true)

# prior setting

a <- 0.1 # shape1
b <- 0.1 # shape2

# MAP estimator (analytically)

theta.map <- (sum(Y) -0.9)/(M*n +a+b-2)
theta.map

# MAP estimator (numerically)

nll <- function(theta){
  post_log_likelihood <- (a+sum(Y)-1)*log(theta) + (n*M+b-sum(Y)-1)*log(1-theta)
  return (-post_log_likelihood)
  }


theta.map.numeric <- optimize( nll,c(0,1))$minimum
theta.map.numeric

# uncertainty estimates (analytically)

theta.hess <- optimHess(theta.map,nll)
theta.hess

# uncertainty estimates (numerically)

theta.hess.numeric <- c(optimHess(theta.map.numeric, nll))
theta.hess.numeric

# true 95% CI comparison

qbeta(c(0.025, 0.975), a+sum(Y)-1, M*n+b-sum(Y)-1)

# Normal approximation
qnorm(c(0.025, 0.975), mean = theta.map, sd = sqrt(1/theta.hess))
